---
title: "COMPSCIX 415.2 Final"
author: "Elena Tolpygo Cranley"
date: "4/4/2018"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, warning=FALSE, message=FALSE}
library(tidyverse)
library(modelr)
library(rpart)
library(partykit)
library(randomForest)
library(ROCR)
```

# Bootstrapping (10 points)

## 1. Follow these steps:
- Load the train.csv dataset into R.
- Convert all character columns into unordered factors.
- Convert the Survived column into an unordered factor because it is loaded as an integer by default. 
- Take a glimpse of your data to confirm that all of the columns were converted correctly.

```{r}
train <- read.csv("~/Documents/Box\ Sync/^edu/compscix-415-2-assignments/train.csv") 
train <- train %>% mutate(Survived=as_factor(case_when(Survived == 0 ~ 'no', Survived == 1 ~ 'yes'))) 
glimpse(as_tibble(train))
```

## 2. Use the code below to take 100 bootstrap samples of your data. Confirm that the result is a tibble with a list column of resample objects - each resample object is a bootstrap sample of the titanic dataset.

```{r}
titanic_boot <- bootstrap(data = train, n = 100)
glimpse(titanic_boot)
```

```{r}
titanic_boot$strap[[1]] %>% #extract the first set
  as.tibble() %>% #convert to tibble
  glimpse()
```


## 3. Confirm that some of your bootstrap samples are in fact bootstrap samples (meaning they should have some rows that are repeated). You can use the n_distinct() function from dplyr to see that your samples have different numbers of unique rows. Use the code below to help you extract some of the resample objects from the strap column (which is an R list), convert them to tibbles, and then count distinct rows. Use the code below, no changes necessary.

```{r}
# since the strap column of titanic_boot is a list, we can
# extract the resampled data using the double brackets [[]], # and just pick out a few of them to compare the number of
# distinct rows
as.tibble(titanic_boot$strap[[1]]) %>% n_distinct()
as.tibble(titanic_boot$strap[[2]]) %>% n_distinct() 
as.tibble(titanic_boot$strap[[3]]) %>% n_distinct()
```

## 4. Now, let’s demonstrate the Central Limit Theorem using the Age column. We’ll iterate through all 100 bootstrap samples, take the mean of Age, and collect the results.

- We will define our own function to pull out the mean of Age from each bootstrap sample and
- create our own for loop to iterate through.
Use the code below and fill in the blanks.

```{r}
age_mean <- function(sample) {
  data <- as.tibble(sample) # convert input data set to a tibble
  mean_age <- mean(data$Age, na.rm = TRUE) # take the mean of Age, remove NAs 
  return(mean_age) # return the mean value of Age from data
}

# loop through the 100 bootstrap samples and use the age_mean() # function
all_means <- rep(NA, 100)

# start the loop
for(i in 1:100) {
  all_means[i] <- age_mean(titanic_boot$strap[[i]])
}

# take a look at some of the means you calculated from your samples
head(all_means)

# convert to a tibble so we can use if for plotting
all_means <- tibble(all_means = all_means)
```

## 5. Plot a histogram of all_means.
```{r}
all_means %>% ggplot(aes(x=all_means)) + geom_histogram(bins = 25) + xlab('Means')
```

## 6. Find the standard error of the sample mean of Age using your boostrap sample means. Compare the empirical standard error to the theoretical standard error. Recall that the theoretical standard error is given by: SE = σ / √n where σ is the standard deviation of Age and n is the size of our sample.

```{r}
(se_Age_bootstrap = sd(all_means$all_means))
(se_Age_empirical = sd(train$Age, na.rm = TRUE) / sqrt(nrow(train)))
```


# Random forest (10 points)
On the last homework, we fit a decision tree to the Titanic data set to predict the probability of survival given the features. This week we’ll use the random forest and compare our results to the decision tree.

## 1. Randomly split your data into training and testing using the code below so that we all have the same sets.

```{r}
set.seed(987)
model_data <- resample_partition(train, c(test = 0.3, train = 0.7))
train_set <- as.tibble(model_data$train) 
test_set <- as.tibble(model_data$test)
```

## 2. Fit a decision tree to train_set using the rpart package, and using Pclass, Sex, Age, SibSp, Parch, Fare, Embarked as the features.
- Plot the tree using the partykit package.
- What do you notice about this tree compared to the one from last week which only contained three
features?

```{r}
tree <- rpart(Survived ~ Pclass+Sex+Age+SibSp+Parch+Fare+Embarked, data = train_set)
plot(as.party(tree))
```

Although this tree has more possible features, it uses only 4 of them, and the main branches are the same as in the 3-featured tree: Sex, followed by Pclass and Fare for women (and a new main branch of Age for men). Some branches are still surprising, such as women in third class who had paid a Fare of >= 22.73 tended not to survive, but those who paid < 7.88 generally survived. So using more features to fit the tree does not necessarily improve accuracy, and trees will have significant variation depending on the chosen sample.

## 3. Fit a random forest to train_set using the randomForest package, and using Pclass, Sex, Age, SibSp, Parch, Fare, Embarked as the features. We’ll use 500 trees and sample four features at each split. Use the code below and fill in the blanks.

```{r}
rf_mod <- randomForest(Survived ~ Pclass+Sex+Age+SibSp+Parch+Fare+Embarked,
                         data = train_set,
                         ntrees = 500,
                         mtry = 4,
                         na.action = na.roughfix)
```

## 4. Compare the performance of the decision tree with the random forest using the ROCR package and the AUC. Which model performs the best?

```{r}
rf_preds <- predict(rf_mod, newdata = test_set, type = 'prob')[,2] 
tree_preds <- predict(tree, newdata = test_set)[,2]

pred_rf <- prediction(predictions = rf_preds, labels = test_set$Survived) 
pred_tree <- prediction(predictions = tree_preds, labels = test_set$Survived)


# get the FPR and TPR for the logistic model
# recall that the ROC curve plots the FPR on the x-axis
perf_rf <- performance(pred_rf, measure = 'tpr', x.measure = 'fpr')
perf_rf_tbl <- tibble(perf_rf@x.values[[1]], perf_rf@y.values[[1]])

# Change the names of the columns of the tibble
names(perf_rf_tbl) <- c('fpr', 'tpr')

# get the FPR and TPR for the tree model
perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr')
perf_tree_tbl <- tibble(perf_tree@x.values[[1]], perf_tree@y.values[[1]])

# Change the names of the columns of the tibble
names(perf_tree_tbl) <- c('fpr', 'tpr')


# Plotting function for plotting a nice ROC curve using ggplot
plot_roc <- function(perf_tbl) {
  p <- ggplot(data = perf_tbl, aes(x = fpr, y = tpr)) +
  geom_line(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, lty = 3) +
  labs(x = 'False positive rate', y = 'True positive rate') +
  theme_bw()
  return(p)
}

# Create the ROC curves using the function we created above
plot_roc(perf_rf_tbl)
plot_roc(perf_tree_tbl)

# calculate the AUCs
auc_rf <- performance(pred_rf, measure = "auc")
auc_tree <- performance(pred_tree, measure = "auc")

# extract the AUC values
auc_rf@y.values[[1]]
auc_tree@y.values[[1]]
```

The random forest model is performing better, as seen in both the ROC curves and the AUC values.

## 5. Plot the ROC curves for the decision tree and the random forest above, on the same plot with a legend that differentiates and specifies which curve belongs to which model. Use the code below to get you started. Hints:
- You will have to modify the plot_roc() function to plot the two curves together with different colors and a legend.
- This is easier to do if the data for plotting the two curves are in one tibble. You can combine tibbles using the bind_rows() function.

```{r}


combo <- bind_rows(list(perf_rf_tbl, perf_tree_tbl), .id = "model") %>%
  mutate(model = fct_recode(model,
    "forest"    = "1",
    "tree"      = "2"
  ))

plot_roc <- function(perf_tbl) {
  p <- ggplot(data = perf_tbl, aes(x = fpr, y = tpr, color=model)) + geom_line() +
  geom_abline(intercept = 0, slope = 1, lty = 3) +
  labs(x = 'False positive rate', y = 'True positive rate') + theme_bw()
  return(p)
}

plot_roc(combo)
```

## 6. Answer these questions about the ROC curves:

### which model performs better: decision tree or random forest?

Random forest performs better: its ROC curve is further away from the random guessing line with a larger AUC.

### what is the approximate false positive rate, for both the decision tree and the random forest, if we attain a true positive rate of approximately 0.75? Answers do not need to be exact - just ballpark it by looking at the plots.

- decision tree: 0.31
- random forest: 0.14
