---
title: "COMPSCIX 415.2 Homework 8"
author: "Elena Tolpygo Cranley"
date: "3/26/2018"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, warning=FALSE, message=FALSE}
library(tidyverse)
library(forcats)
library(rpart)
library(partykit)
library(ROCR)
library(broom)
```

## Exercise 1
Load the ass8_train.csv dataset into R. How many observations and columns are there? Convert the target variable to a factor because it will be loaded into R as an integer by default.

```{r}
train <- read.csv("~/Documents/Box\ Sync/^edu/compscix-415-2-assignments/ass8_train.csv") 
train <- train %>% mutate(Survived=as_factor(case_when(Survived == 0 ~ 'no', Survived == 1 ~ 'yes'))) 
glimpse(as_tibble(train))
```
  
Observations: 891
Variables: 12

## Exercise 2
Our first step is to randomly split the data into train and test datasets. We will use a 70/30 split, and use the random seed of 29283 so that we all should get the same training and test set.

```{r}
set.seed(29283)
train_set <- train %>% sample_frac(0.7)
test_set <- train %>% filter(!(PassengerId %in% train_set$PassengerId))
```

## Exercise 3
Our target is called Survived. First, fit a logistic regression model using Pclass, Sex, Fare as your three features. Fit the model using the glm() function. Ask yourself these questions before fitting the model:

### What kind of relationship will these features have with the probability of survival?
```{r}
as_tibble(train_set) %>% ggplot(aes(x=Survived, fill=factor(Pclass), group=factor(Pclass))) + geom_bar()
```

Passengers in the 1st class were much more likely to survive than passengers in the 3rd class.

```{r}
as_tibble(train_set) %>% ggplot(aes(x=Survived, fill=Sex, group=Sex)) + geom_bar()
```

Women were much more likely to survive than men.

```{r}
as_tibble(train_set) %>% ggplot(aes(x=Survived, y=Fare)) + geom_boxplot()
```

Passengers who survived tended to have paid a higher ticket price.


### Are these good features, given the problem we are trying to solve?

```{r}
as_tibble(train_set) %>% ggplot(aes(x=Pclass, y=Fare, group=Pclass)) + geom_boxplot()
```

All three features appear to have predictive capacity on the Survival outcome; however, as ticket price is also strongly associated with passenger class, using both features might be redundant.

```{r}
as_tibble(train_set) %>% ggplot(aes(x=Pclass, y=Fare, group=interaction(Survived, Pclass), fill=Survived), position="dodge") + geom_boxplot()
```

It appears that only for passengers in the first class was there a higher association between higher ticket price and survival likelihood; otherwise, passenger class alone is a better predictor.

```{r}
as_tibble(train_set) %>% filter(Pclass>1) %>% ggplot(aes(x=factor(Pclass), y=Fare, group=interaction(Survived, Pclass), fill=Survived), position="dodge") + geom_boxplot()
```



```{r}
# Fit a model with intercept only
mod_1 <- glm(Survived ~ Pclass+Sex+Fare, data = train_set, family = 'binomial')

# take a look at the features and coefficients
tidy(mod_1)
```


After fitting the model, output the coefficients using the broom package and answer these questions:

### How would you interpret the coefficients?

The coefficients agree with the previous observations:

- having a class with a higher numeric value (that is, a lower class) is negatively associated with survival;
- being male is strongly negatively associated with survival;
- for each dollar more paid in ticket fare, likelihood of survival increased by about 1.8% - although this is a very weak association since the p-value is quite high. If we model using ticket fare without passenger class, the p-value is much better.

### Are the features significant?

Gender is very significant; passenger class and ticket price are significant separately (and class is more significant than ticket price), but when both are used to fit the model, ticket price is not significant.


## Exercise 4
Now, let’s fit a model using a classification tree, using the same features and plot the final decision tree. Use the code below for help.

```{r}
tree_mod <- rpart(Survived ~ Pclass+Sex+Fare, data = train_set)
plot(as.party(tree_mod))
```


Answer these questions:

### Describe in words one path a Titanic passenger might take down the tree. (Hint: look at your tree, choose a path from the top to a terminal node, and describe the path like this - *a male passenger who paid a fare > 30 and was in first class has a high probability of survival*)

A female in second class or better had a very high likelihood of survival.


### Does anything surprise you about the fitted tree?

If a female was in third class and paid:

- more than 23.7 - very unlikely to survive;
- between 23.7 and 15 - fairly likely to survive;
- between 15 and 7.888 - more likely to not survive;
- less than 7.88 - fairly likely to survive.

In other words, for women in third class, having paid a higher ticket price did not lead to higher likelihood of survival. Women who paid less than 7.88 or 23.7 and 15 were likely to survive, but those who paid between 15 and 7.888 or more than 23.7 were not likely to survive. This is probably(?) an artifact of the training data selection and relatively small numbers in each outcome bin.


## Exercise 5

Evaluate both the logistic regression model and classification tree on the test_set. First, use the predict() function to get the model predictions for the testing set. Use the code below for help.

```{r}
test_logit <- predict(mod_1, newdata = test_set, type = 'response')
test_tree <- predict(tree_mod, newdata = test_set)[,2]
```

### (a) Next, we will plot the ROC curves from both models using the code below. Don’t just copy and paste the code. Go through it line by line and see what it is doing. Recall that predictions from your decision tree are given as a two column matrix.

```{r}
# create the prediction objects for both models
pred_logit <- prediction(predictions = test_logit, labels = test_set$Survived)
pred_tree <- prediction(predictions = test_tree, labels = test_set$Survived)

# get the FPR and TPR for the logistic model
# recall that the ROC curve plots the FPR on the x-axis
perf_logit <- performance(pred_logit, measure = 'tpr', x.measure = 'fpr')
perf_logit_tbl <- tibble(perf_logit@x.values[[1]], perf_logit@y.values[[1]])

# Change the names of the columns of the tibble
names(perf_logit_tbl) <- c('fpr', 'tpr')
# get the FPR and TPR for the tree model
perf_tree <- performance(pred_tree, measure = 'tpr', x.measure = 'fpr')
perf_tree_tbl <- tibble(perf_tree@x.values[[1]], perf_tree@y.values[[1]])

# Change the names of the columns of the tibble
names(perf_tree_tbl) <- c('fpr', 'tpr')

# Plotting function for plotting a nice ROC curve using ggplot
plot_roc <- function(perf_tbl) {
  p <- ggplot(data = perf_tbl, aes(x = fpr, y = tpr)) +
  geom_line(color = 'blue') +
  geom_abline(intercept = 0, slope = 1, lty = 3) +
  labs(x = 'False positive rate', y = 'True positive rate') +
  theme_bw()
  return(p)
}

# Create the ROC curves using the function we created above
plot_roc(perf_logit_tbl)
plot_roc(perf_tree_tbl)
```

### (b) Now, use the performance() function to calculate the area under the curve (AUC) for both ROC curves. Check ?performance for help on plugging in the right measure argument.

```{r}
# calculate the AUC
auc_logit <- performance(pred_logit, measure = "auc")
auc_tree <- performance(pred_tree, measure = "auc")

# extract the AUC value
auc_logit@y.values[[1]]
auc_tree@y.values[[1]]
```

#### What do you notice about the ROC curves and the AUC values? Are the models performing well? Is the logistic regression model doing better, worse, or about the same as the classification tree?

Both models are performing reasonably well, since both ROC curves are well above the random guessing line and both AUCs are well over 0.5. The logistic regression model is performing better, since the AUC is closer to 1.


### (c) Lastly, pick a probability cutoff by looking at the ROC curves. You pick, there’s no right answer (but there is a wrong answer - make sure to pick something between 0 and 1). Using that probability cutoff, create the confusion matrix for each model by following these steps:

#### 1. Pick a cutoff value.

From https://hopstat.wordpress.com/2014/12/19/a-small-introduction-to-the-rocr-package/ :
"In the performance object, we have the slot x.values, which corresponds to the cutoff in this case, and y.values, which corresponds to the accuracy of each cutoff. We'll grab the index for maximum accuracy and then grab the corresponding cutoff:"


##### logistic regression:

```{r}
acc.perf_logit = performance(pred_logit, measure = "acc")
plot(acc.perf_logit)
```

```{r}
ind = which.max( slot(acc.perf_logit, "y.values")[[1]] )
acc = slot(acc.perf_logit, "y.values")[[1]][ind]
cutoff = slot(acc.perf_logit, "x.values")[[1]][ind]
print(c(accuracy= acc, cutoff = cutoff))
```

##### classification tree:

```{r}
acc.perf_tree = performance(pred_tree, measure = "acc")
plot(acc.perf_tree)
```

```{r}
ind = which.max( slot(acc.perf_tree, "y.values")[[1]] )
acc = slot(acc.perf_tree, "y.values")[[1]][ind]
cutoff = slot(acc.perf_tree, "x.values")[[1]][ind]
print(c(accuracy= acc, cutoff = cutoff))
```


#### 2. Append the predicted probability values from each model (you created these at the beginning of Exercise 5) to your test_set tibble using mutate().

```{r}
test_set <- test_set %>% mutate(logit_value=predict(mod_1, newdata = test_set, type = 'response'), tree_value=predict(tree_mod, newdata = test_set)[,2])
```



#### 3. Create a new column for the predicted class from each model using mutate() and case_when(). Your new predicted class columns can have two possible values: yes or no which represents whether or not the passenger is predicted to have survived or not given the predicted probability.

```{r}
test_set <- test_set %>% mutate(predict_logit=case_when((logit_value >= 0.806) ~ 'yes', (logit_value < 0.806) ~ 'no'), predict_tree=case_when((tree_value >= 0.724) ~ 'yes', (tree_value < 0.724) ~ 'no'))
```


#### 4. You should now have 4 extra columns added to your test_set tibble, two columns of predicted probabilities, and two columns of the predicted categories based on your probability cutoff.

```{r}
glimpse(as_tibble(test_set))
```


#### 5. Now create the table using the code below:

```{r}
test_set %>% count(predict_logit, Survived) %>% spread(Survived, n)
test_set %>% count(predict_tree, Survived) %>% spread(Survived, n)
```

